package net.i2p.router.networkdb.kademlia;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Set;
import net.i2p.crypto.SigType;
import net.i2p.data.DatabaseEntry;
import net.i2p.data.Destination;
import net.i2p.data.Hash;
import net.i2p.data.TunnelId;
import net.i2p.data.i2np.DatabaseLookupMessage;
import net.i2p.data.i2np.DatabaseStoreMessage;
import net.i2p.data.router.RouterInfo;
import net.i2p.data.router.RouterKeyGenerator;
import net.i2p.router.Job;
import net.i2p.router.JobImpl;
import net.i2p.router.OutNetMessage;
import net.i2p.router.Router;
import net.i2p.router.RouterContext;
import net.i2p.util.ConcurrentHashSet;
import net.i2p.util.Log;
import net.i2p.util.SystemVersion;
public class FloodfillNetworkDatabaseFacade extends KademliaNetworkDatabaseFacade {
public static final char CAPABILITY_FLOODFILL = 'f';
private final Map<Hash, FloodSearchJob> _activeFloodQueries;
private boolean _floodfillEnabled;
private final Set<Hash> _verifiesInProgress;
private FloodThrottler _floodThrottler;
private LookupThrottler _lookupThrottler;
private final Job _ffMonitor;
public static final int MAX_TO_FLOOD = 3;
private static final int FLOOD_PRIORITY = OutNetMessage.PRIORITY_NETDB_FLOOD;
private static final int FLOOD_TIMEOUT = 30*1000;
private static final long NEXT_RKEY_RI_ADVANCE_TIME = 45*60*1000;
private static final long NEXT_RKEY_LS_ADVANCE_TIME = 10*60*1000;
private static final int NEXT_FLOOD_QTY = 2;
public FloodfillNetworkDatabaseFacade(RouterContext context) {
super(context);
_activeFloodQueries = new HashMap<Hash, FloodSearchJob>();
_verifiesInProgress = new ConcurrentHashSet<Hash>(8);
_context.statManager().createRequiredRateStat("netDb.successTime", "Time for successful lookup (ms)", "NetworkDatabase", new long[] { 60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.failedTime", "How long a failed search takes", "NetworkDatabase", new long[] { 60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.failedRetries", "How many additional queries for an iterative search", "NetworkDatabase", new long[] { 60*60*1000l });
_context.statManager().createRateStat("netDb.successRetries", "How many additional queries for an iterative search", "NetworkDatabase", new long[] { 60*60*1000l });
_context.statManager().createRateStat("netDb.failedAttemptedPeers", "How many peers we sent a search to when the search fails", "NetworkDatabase", new long[] { 10*60*1000l });
_context.statManager().createRateStat("netDb.successPeers", "How many peers are contacted in a successful search", "NetworkDatabase", new long[] { 60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.failedPeers", "How many peers fail to respond to a lookup?", "NetworkDatabase", new long[] { 60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.searchCount", "Overall number of searches sent", "NetworkDatabase", new long[] { 5*60*1000l, 10*60*1000l, 60*60*1000l, 3*60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.searchMessageCount", "Overall number of mesages for all searches sent", "NetworkDatabase", new long[] { 5*60*1000l, 10*60*1000l, 60*60*1000l, 3*60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.searchReplyValidated", "How many search replies we get that we are able to validate (fetch)", "NetworkDatabase", new long[] { 5*60*1000l, 10*60*1000l, 60*60*1000l, 3*60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.searchReplyNotValidated", "How many search replies we get that we are NOT able to validate (fetch)", "NetworkDatabase", new long[] { 5*60*1000l, 10*60*1000l, 60*60*1000l, 3*60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.searchReplyValidationSkipped", "How many search replies we get from unreliable peers that we skip?", "NetworkDatabase", new long[] { 5*60*1000l, 10*60*1000l, 60*60*1000l, 3*60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.republishQuantity", "How many peers do we need to send a found leaseSet to?", "NetworkDatabase", new long[] { 10*60*1000l, 60*60*1000l, 3*60*60*1000l, 24*60*60*1000l });
_context.statManager().createRateStat("netDb.RILookupDirect", "Was an iterative RI lookup sent directly?", "NetworkDatabase", new long[] { 60*60*1000 });
_ffMonitor = new FloodfillMonitorJob(_context, this);
}
@Override
public synchronized void startup() {
super.startup();
_context.jobQueue().addJob(_ffMonitor);
_lookupThrottler = new LookupThrottler();
Job rrj = new RefreshRoutersJob(_context, this);
rrj.getTiming().setStartAfter(_context.clock().now() + 5*60*1000);
_context.jobQueue().addJob(rrj);
}
@Override
protected void createHandlers() {
_context.inNetMessagePool().registerHandlerJobBuilder(DatabaseLookupMessage.MESSAGE_TYPE, new FloodfillDatabaseLookupMessageHandler(_context, this));
_context.inNetMessagePool().registerHandlerJobBuilder(DatabaseStoreMessage.MESSAGE_TYPE, new FloodfillDatabaseStoreMessageHandler(_context, this));
}
@Override
public synchronized void shutdown() {
if (_floodfillEnabled &&
(!_context.getBooleanProperty(FloodfillMonitorJob.PROP_FLOODFILL_PARTICIPANT) ||
!(_context.router().scheduledGracefulExitCode() == Router.EXIT_HARD_RESTART ||
_context.router().scheduledGracefulExitCode() == Router.EXIT_GRACEFUL_RESTART))) {
_floodfillEnabled = false;
_context.router().rebuildRouterInfo(true);
RouterInfo local = _context.router().getRouterInfo();
if (local != null && _context.router().getUptime() > PUBLISH_JOB_DELAY) {
flood(local);
try {
Thread.sleep(3000);
} catch (InterruptedException ie) {}
}
}
_context.jobQueue().removeJob(_ffMonitor);
super.shutdown();
}
static final long PUBLISH_TIMEOUT = 90*1000;
@Override
public void publish(RouterInfo localRouterInfo) throws IllegalArgumentException {
if (localRouterInfo == null) throw new IllegalArgumentException("impossible: null localRouterInfo?");
if (_context.router().isHidden()) return; 
super.publish(localRouterInfo);
if (!isInitialized()) {
if (_log.shouldWarn())
_log.warn("publish() before initialized: " + localRouterInfo, new Exception("I did it"));
return;
}
if (localRouterInfo.getAddresses().isEmpty())
return;
_log.info("Publishing our RI");
sendStore(localRouterInfo.getIdentity().calculateHash(), localRouterInfo, null, null, PUBLISH_TIMEOUT, null);
}
@Override
public void sendStore(Hash key, DatabaseEntry ds, Job onSuccess, Job onFailure, long sendTimeout, Set<Hash> toIgnore) {
if (floodfillEnabled() && (ds.getType() == DatabaseEntry.KEY_TYPE_ROUTERINFO)) {
flood(ds);
if (onSuccess != null)
_context.jobQueue().addJob(onSuccess);
} else {
_context.jobQueue().addJob(new FloodfillStoreJob(_context, this, key, ds, onSuccess, onFailure, sendTimeout, toIgnore));
}
}
boolean shouldThrottleFlood(Hash key) {
return _floodThrottler != null && _floodThrottler.shouldThrottle(key);
}
boolean shouldThrottleLookup(Hash from, TunnelId id) {
return _lookupThrottler == null || _lookupThrottler.shouldThrottle(from, id);
}
public boolean floodConditional(DatabaseEntry ds) {
if (!floodfillEnabled())
return false;
if (shouldThrottleFlood(ds.getHash())) {
_context.statManager().addRateData("netDb.floodThrottled", 1);
return false;
}
flood(ds);
return true;
}
public void flood(DatabaseEntry ds) {
Hash key = ds.getHash();
RouterKeyGenerator gen = _context.routerKeyGenerator();
Hash rkey = gen.getRoutingKey(key);
FloodfillPeerSelector sel = (FloodfillPeerSelector)getPeerSelector();
final int type = ds.getType();
final boolean isls = ds.isLeaseSet();
final boolean isls2 = isls && type != DatabaseEntry.KEY_TYPE_LEASESET;
final SigType lsSigType = (isls && type != DatabaseEntry.KEY_TYPE_ENCRYPTED_LS2) ?
ds.getKeysAndCert().getSigningPublicKey().getType() :
null;
int max = MAX_TO_FLOOD;
if (type == DatabaseEntry.KEY_TYPE_ENCRYPTED_LS2)
max *= 4;
else if (isls2)
max *= 2;
List<Hash> peers = sel.selectFloodfillParticipants(rkey, max, getKBuckets());
long until = gen.getTimeTillMidnight();
if (until < NEXT_RKEY_LS_ADVANCE_TIME ||
(type == DatabaseEntry.KEY_TYPE_ROUTERINFO && until < NEXT_RKEY_RI_ADVANCE_TIME)) {
Hash nkey = gen.getNextRoutingKey(key);
List<Hash> nextPeers = sel.selectFloodfillParticipants(nkey, NEXT_FLOOD_QTY, getKBuckets());
int i = 0;
for (Hash h : nextPeers) {
if (h.equals(key))
continue;
if (!peers.contains(h)) {
peers.add(h);
i++;
}
if (i >= MAX_TO_FLOOD)
break;
}
if (i > 0) {
max += i;
if (_log.shouldInfo())
_log.info("Flooding the entry for " + key + " to " + i + " more, just before midnight");
}
}
int flooded = 0;
for (int i = 0; i < peers.size(); i++) {
Hash peer = peers.get(i);
RouterInfo target = lookupRouterInfoLocally(peer);
if (!shouldFloodTo(key, type, lsSigType, peer, target)) {
if (_log.shouldDebug())
_log.debug("Too old, not flooding " + key.toBase64() + " to " + peer.toBase64());
continue;
}
DatabaseStoreMessage msg = new DatabaseStoreMessage(_context);
msg.setEntry(ds);
OutNetMessage m = new OutNetMessage(_context, msg, _context.clock().now()+FLOOD_TIMEOUT, FLOOD_PRIORITY, target);
Job floodFail = new FloodFailedJob(_context, peer);
m.setOnFailedSendJob(floodFail);
Job floodGood = new FloodSuccessJob(_context, peer);
m.setOnSendJob(floodGood);
_context.commSystem().processMessage(m);
flooded++;
if (_log.shouldLog(Log.INFO))
_log.info("Flooding the entry for " + key.toBase64() + " to " + peer.toBase64());
if (flooded >= MAX_TO_FLOOD)
break;
}
if (_log.shouldLog(Log.INFO))
_log.info("Flooded the data to " + flooded + " of " + peers.size() + " peers");
}
private boolean shouldFloodTo(Hash key, int type, SigType lsSigType, Hash peer, RouterInfo target) {
if ( (target == null) || (_context.banlist().isBanlisted(peer)) )
return false;
if (type == DatabaseEntry.KEY_TYPE_ROUTERINFO && peer.equals(key))
return false;
if (peer.equals(_context.routerHash()))
return false;
if (type != DatabaseEntry.KEY_TYPE_ROUTERINFO && type != DatabaseEntry.KEY_TYPE_LEASESET &&
!StoreJob.shouldStoreLS2To(target))
return false;
if ((type == DatabaseEntry.KEY_TYPE_ENCRYPTED_LS2 ||
lsSigType == SigType.RedDSA_SHA512_Ed25519) &&
!StoreJob.shouldStoreEncLS2To(target))
return false;
if (!StoreJob.shouldStoreTo(target))
return false;
return true;
}
private static class FloodFailedJob extends JobImpl {
private final Hash _peer;
public FloodFailedJob(RouterContext ctx, Hash peer) {
super(ctx);
_peer = peer;
}
public String getName() { return "Flood failed"; }
public void runJob() {
getContext().profileManager().dbStoreFailed(_peer);
}
}
private static class FloodSuccessJob extends JobImpl {
private final Hash _peer;
public FloodSuccessJob(RouterContext ctx, Hash peer) {
super(ctx);
_peer = peer;
}
public String getName() { return "Flood succeeded"; }
public void runJob() {
getContext().profileManager().dbStoreSuccessful(_peer);
}
}
@Override
protected PeerSelector createPeerSelector() { return new FloodfillPeerSelector(_context); }
public synchronized void setFloodfillEnabled(boolean yes) {
if (yes != _floodfillEnabled) {
_context.jobQueue().removeJob(_ffMonitor);
_ffMonitor.getTiming().setStartAfter(_context.clock().now() + 1000);
_context.jobQueue().addJob(_ffMonitor);
}
}
synchronized void setFloodfillEnabledFromMonitor(boolean yes) {
_floodfillEnabled = yes;
if (yes && _floodThrottler == null) {
_floodThrottler = new FloodThrottler();
_context.statManager().createRateStat("netDb.floodThrottled", "How often do we decline to flood?", "NetworkDatabase", new long[] { 60*60*1000l });
_context.statManager().createRateStat("netDb.storeFloodNew", "How long it takes to flood out a newly received entry?", "NetworkDatabase", new long[] { 60*60*1000l });
_context.statManager().createRateStat("netDb.storeFloodOld", "How often we receive an old entry?", "NetworkDatabase", new long[] { 60*60*1000l });
}
}
@Override
public boolean floodfillEnabled() { return _floodfillEnabled; }
public static boolean isFloodfill(RouterInfo peer) {
if (peer == null) return false;
String caps = peer.getCapabilities();
return caps.indexOf(FloodfillNetworkDatabaseFacade.CAPABILITY_FLOODFILL) >= 0;
}
public List<RouterInfo> getKnownRouterData() {
List<RouterInfo> rv = new ArrayList<RouterInfo>();
DataStore ds = getDataStore();
if (ds != null) {
for (DatabaseEntry o : ds.getEntries()) {
if (o.getType() == DatabaseEntry.KEY_TYPE_ROUTERINFO)
rv.add((RouterInfo)o);
}
}
return rv;
}
@Override
SearchJob search(Hash key, Job onFindJob, Job onFailedLookupJob, long timeoutMs, boolean isLease) {
return search(key, onFindJob, onFailedLookupJob, timeoutMs, isLease, null);
}
SearchJob search(Hash key, Job onFindJob, Job onFailedLookupJob, long timeoutMs, boolean isLease,
Hash fromLocalDest) {
if (key == null) throw new IllegalArgumentException("searchin for nothin, eh?");
boolean isNew = false;
FloodSearchJob searchJob;
synchronized (_activeFloodQueries) {
searchJob = _activeFloodQueries.get(key);
if (searchJob == null) {
searchJob = new IterativeSearchJob(_context, this, key, onFindJob, onFailedLookupJob, (int)timeoutMs,
isLease, fromLocalDest);
_activeFloodQueries.put(key, searchJob);
isNew = true;
}
}
if (isNew) {
if (_log.shouldLog(Log.DEBUG))
_log.debug("this is the first search for that key, fire off the FloodSearchJob");
_context.jobQueue().addJob(searchJob);
} else {
if (_log.shouldLog(Log.INFO))
_log.info("Deferring flood search for " + key.toBase64() + " with " + _activeFloodQueries.size() + " in progress");
searchJob.addDeferred(onFindJob, onFailedLookupJob, timeoutMs, isLease);
_context.statManager().addRateData("netDb.lookupDeferred", 1, searchJob.getExpiration()-_context.clock().now());
}
return null;
}
void searchFull(Hash key, List<Job> onFind, List<Job> onFailed, long timeoutMs, boolean isLease) {
synchronized (_activeFloodQueries) { _activeFloodQueries.remove(key); }
Job find = null;
Job fail = null;
if (onFind != null) {
synchronized (onFind) {
if (!onFind.isEmpty())
find = onFind.remove(0);
}
}
if (onFailed != null) {
synchronized (onFailed) {
if (!onFailed.isEmpty())
fail = onFailed.remove(0);
}
}
SearchJob job = super.search(key, find, fail, timeoutMs, isLease);
if (job != null) {
if (_log.shouldLog(Log.INFO))
_log.info("Floodfill search timed out for " + key.toBase64() + ", falling back on normal search (#"
+ job.getJobId() + ") with " + timeoutMs + " remaining");
long expiration = timeoutMs + _context.clock().now();
List<Job> removed = null;
if (onFind != null) {
synchronized (onFind) {
removed = new ArrayList(onFind);
onFind.clear();
}
for (int i = 0; i < removed.size(); i++)
job.addDeferred(removed.get(i), null, expiration, isLease);
removed = null;
}
if (onFailed != null) {
synchronized (onFailed) {
removed = new ArrayList(onFailed);
onFailed.clear();
}
for (int i = 0; i < removed.size(); i++)
job.addDeferred(null, removed.get(i), expiration, isLease);
removed = null;
}
}
}
void complete(Hash key) {
synchronized (_activeFloodQueries) { _activeFloodQueries.remove(key); }
}
public List<Hash> getFloodfillPeers() {
FloodfillPeerSelector sel = (FloodfillPeerSelector)getPeerSelector();
return sel.selectFloodfillParticipants(getKBuckets());
}
boolean isVerifyInProgress(Hash h) {
return _verifiesInProgress.contains(h);
}
void verifyStarted(Hash h) {
_verifiesInProgress.add(h);
}
void verifyFinished(Hash h) {
_verifiesInProgress.remove(h);
}
protected final static int MIN_ACTIVE_PEERS = 5;
private static final int MAX_DB_BEFORE_SKIPPING_SEARCH;
static {
long maxMemory = SystemVersion.getMaxMemory();
MAX_DB_BEFORE_SKIPPING_SEARCH = (int) Math.max(250l, Math.min(1250l, maxMemory / ((32 * 1024 * 1024l) / 250)));
}
@Override
protected void lookupBeforeDropping(Hash peer, RouterInfo info) {
if (info.getNetworkId() == _networkID &&
(getKBucketSetSize() < MIN_REMAINING_ROUTERS ||
_context.router().getUptime() < DONT_FAIL_PERIOD ||
_context.commSystem().countActivePeers() <= MIN_ACTIVE_PEERS)) {
if (_log.shouldInfo())
_log.info("Not failing " + peer.toBase64() + " as we are just starting up or have problems");
return;
}
if (_floodfillEnabled ||
_context.jobQueue().getMaxLag() > 500 ||
_context.banlist().isBanlistedForever(peer) ||
getKBucketSetSize() > MAX_DB_BEFORE_SKIPPING_SEARCH) {
super.lookupBeforeDropping(peer, info);
return;
}
search(peer, new DropLookupFoundJob(_context, peer, info), new DropLookupFailedJob(_context, peer, info), 10*1000, false);
}
private class DropLookupFailedJob extends JobImpl {
private final Hash _peer;
private final RouterInfo _info;
public DropLookupFailedJob(RouterContext ctx, Hash peer, RouterInfo info) {
super(ctx);
_peer = peer;
_info = info;
}
public String getName() { return "Lookup on failure of netDb peer timed out"; }
public void runJob() {
dropAfterLookupFailed(_peer);
}
}
private class DropLookupFoundJob extends JobImpl {
private final Hash _peer;
private final RouterInfo _info;
public DropLookupFoundJob(RouterContext ctx, Hash peer, RouterInfo info) {
super(ctx);
_peer = peer;
_info = info;
}
public String getName() { return "Lookup on failure of netDb peer matched"; }
public void runJob() {
RouterInfo updated = lookupRouterInfoLocally(_peer);
if ( (updated != null) && (updated.getPublished() > _info.getPublished()) ) {
} else {
dropAfterLookupFailed(_peer);
}
}
}
}
