package com.datumbox.framework.core.machinelearning.classification;
import com.datumbox.framework.common.Configuration;
import com.datumbox.framework.common.concurrency.StreamMethods;
import com.datumbox.framework.common.dataobjects.AssociativeArray;
import com.datumbox.framework.core.common.dataobjects.Dataframe;
import com.datumbox.framework.core.common.dataobjects.Record;
import com.datumbox.framework.common.dataobjects.TypeInference;
import com.datumbox.framework.common.storage.interfaces.StorageEngine;
import com.datumbox.framework.core.machinelearning.common.abstracts.AbstractTrainer;
import com.datumbox.framework.core.machinelearning.common.abstracts.algorithms.AbstractNaiveBayes;
import com.datumbox.framework.core.machinelearning.common.interfaces.PredictParallelizable;
import com.datumbox.framework.core.statistics.descriptivestatistics.Descriptives;
import java.util.*;
public class BernoulliNaiveBayes extends AbstractNaiveBayes<BernoulliNaiveBayes.ModelParameters, BernoulliNaiveBayes.TrainingParameters> {
public static class ModelParameters extends AbstractNaiveBayes.AbstractModelParameters {
private static final long serialVersionUID = 1L;
private Map<Object, Double> sumOfLog1minusProb = new HashMap<>(); 
protected ModelParameters(StorageEngine storageEngine) {
super(storageEngine);
}
public Map<Object, Double> getSumOfLog1minusProb() {
return sumOfLog1minusProb;
}
protected void setSumOfLog1minusProb(Map<Object, Double> sumOfLog1minusProb) {
this.sumOfLog1minusProb = sumOfLog1minusProb;
}
}
public static class TrainingParameters extends AbstractNaiveBayes.AbstractTrainingParameters {
private static final long serialVersionUID = 1L;
}
protected BernoulliNaiveBayes(TrainingParameters trainingParameters, Configuration configuration) {
super(trainingParameters, configuration);
}
protected BernoulliNaiveBayes(String storageName, Configuration configuration) {
super(storageName, configuration);
}
protected boolean isBinarized() {
return true;
}
@Override
public PredictParallelizable.Prediction _predictRecord(Record r) {
ModelParameters modelParameters = knowledgeBase.getModelParameters();
Map<List<Object>, Double> logLikelihoods = modelParameters.getLogLikelihoods();
Map<Object, Double> logPriors = modelParameters.getLogPriors();
Set<Object> classesSet = modelParameters.getClasses();
Map<Object, Double> sumOfLog1minusProb = modelParameters.getSumOfLog1minusProb();
Object someClass = classesSet.iterator().next();
AssociativeArray predictionScores = new AssociativeArray(new HashMap<>(logPriors));
for(Map.Entry<Object, Double> entry : sumOfLog1minusProb.entrySet()) {
Object theClass = entry.getKey();
Double value = entry.getValue();
Double previousValue = predictionScores.getDouble(theClass);
predictionScores.put(theClass, previousValue+value);
}
for(Map.Entry<Object, Object> entry : r.getX().entrySet()) {
Object feature = entry.getKey();
if(!logLikelihoods.containsKey(Arrays.asList(feature, someClass))) {
continue;
}
AssociativeArray classLogScoresForThisFeature = new AssociativeArray();
for(Object theClass : classesSet) {
Double logScore = logLikelihoods.get(Arrays.asList(feature, theClass));
classLogScoresForThisFeature.put(theClass, logScore);
}
Double occurrences= TypeInference.toDouble(entry.getValue());
if(occurrences==null || occurrences==0.0) {
continue;
}
for(Map.Entry<Object, Object> entry2 : classLogScoresForThisFeature.entrySet()) {
Object theClass = entry2.getKey();
Double probability = TypeInference.toDouble(entry2.getValue());
Double previousValue = predictionScores.getDouble(theClass);
predictionScores.put(theClass, previousValue + Math.log(probability)-Math.log(1.0-probability));
}
}
Object predictedClass=getSelectedClassFromClassScores(predictionScores);
Descriptives.normalizeExp(predictionScores);
return new PredictParallelizable.Prediction(predictedClass, predictionScores);
}
@Override
protected void _fit(Dataframe trainingData) {
ModelParameters modelParameters = knowledgeBase.getModelParameters();
int n = trainingData.size();
int d = trainingData.xColumnSize();
knowledgeBase.getTrainingParameters().setMultiProbabilityWeighted(false);
Map<List<Object>, Double> likelihoods = modelParameters.getLogLikelihoods();
Map<Object, Double> logPriors = modelParameters.getLogPriors();
Set<Object> classesSet = modelParameters.getClasses();
Map<Object, Double> sumOfLog1minusProb = modelParameters.getSumOfLog1minusProb();
Map<Object, Integer> totalFeatureOccurrencesForEachClass = new HashMap<>();
for(Record r : trainingData) {
Object theClass=r.getY();
if(classesSet.add(theClass)) { 
logPriors.put(theClass, 1.0);
totalFeatureOccurrencesForEachClass.put(theClass, 0);
sumOfLog1minusProb.put(theClass, 0.0);
}
else { 
logPriors.put(theClass,logPriors.get(theClass)+1.0);
}
}
Implementation note:
The code below uses the metadata from the Dataframe to avoid looping through all the data.
This means that if the metadata are stale (contain more columns than the actual data due to
updates/removes) we will initialize more parameters here. Nevertheless this should not have
any effects on the results of the algorithm since the scores will be the same in all classes
and it will be taken care by the normalization.
streamExecutor.forEach(StreamMethods.stream(trainingData.getXDataTypes().keySet().stream(), isParallelized()), feature -> {
for(Object theClass : classesSet) {
List<Object> featureClassTuple = Arrays.asList(feature, theClass);
likelihoods.put(featureClassTuple, 0.0); 
}
});
streamExecutor.forEach(StreamMethods.stream(trainingData.stream(), isParallelized()), r -> {
Object theClass = r.getY();
int sumOfOccurrences = 0;
for(Map.Entry<Object, Object> entry : r.getX().entrySet()) {
Object feature = entry.getKey();
Double occurrences=TypeInference.toDouble(entry.getValue());
if(occurrences!= null && occurrences>0.0) {
List<Object> featureClassTuple = Arrays.asList(feature, theClass);
likelihoods.put(featureClassTuple, likelihoods.get(featureClassTuple)+1.0); 
sumOfOccurrences++;
}
}
synchronized(totalFeatureOccurrencesForEachClass) {
totalFeatureOccurrencesForEachClass.put(theClass, totalFeatureOccurrencesForEachClass.get(theClass)+sumOfOccurrences);
}
});
for(Map.Entry<Object, Double> entry : logPriors.entrySet()) {
Object theClass = entry.getKey();
Double count = entry.getValue();
logPriors.put(theClass, Math.log(count/n));
}
for(Object theClass : classesSet) {
double sumLog1minusP = streamExecutor.sum(StreamMethods.stream(trainingData.getXDataTypes().keySet().stream(), isParallelized()).mapToDouble(feature -> {
List<Object> featureClassTuple = Arrays.asList(feature, theClass);
Double occurrences = likelihoods.get(featureClassTuple);
Double smoothedProbability = (occurrences+1.0)/(totalFeatureOccurrencesForEachClass.get(theClass)+d); 
likelihoods.put(featureClassTuple, smoothedProbability);
double log1minusP = Math.log( 1.0-smoothedProbability );
return log1minusP;
}));
sumOfLog1minusProb.put(theClass, sumOfLog1minusProb.get(theClass) + sumLog1minusP);
}
}
}
