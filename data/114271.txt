package com.datumbox.framework.core.machinelearning.classification;
import com.datumbox.framework.common.Configuration;
import com.datumbox.framework.common.concurrency.ForkJoinStream;
import com.datumbox.framework.common.concurrency.StreamMethods;
import com.datumbox.framework.common.dataobjects.AssociativeArray;
import com.datumbox.framework.core.common.dataobjects.Dataframe;
import com.datumbox.framework.core.common.dataobjects.Record;
import com.datumbox.framework.common.dataobjects.TypeInference;
import com.datumbox.framework.common.storage.interfaces.BigMap;
import com.datumbox.framework.common.storage.interfaces.StorageEngine;
import com.datumbox.framework.common.storage.interfaces.StorageEngine.MapType;
import com.datumbox.framework.common.storage.interfaces.StorageEngine.StorageHint;
import com.datumbox.framework.core.machinelearning.common.abstracts.AbstractTrainer;
import com.datumbox.framework.core.machinelearning.common.abstracts.modelers.AbstractClassifier;
import com.datumbox.framework.core.machinelearning.common.interfaces.PredictParallelizable;
import com.datumbox.framework.core.machinelearning.common.interfaces.TrainParallelizable;
import com.datumbox.framework.core.statistics.descriptivestatistics.Descriptives;
import java.util.Arrays;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.atomic.AtomicBoolean;
public class MaximumEntropy extends AbstractClassifier<MaximumEntropy.ModelParameters, MaximumEntropy.TrainingParameters> implements PredictParallelizable, TrainParallelizable {
public static class ModelParameters extends AbstractClassifier.AbstractModelParameters {
private static final long serialVersionUID = 1L;
@BigMap(keyClass=List.class, valueClass=Double.class, mapType=MapType.HASHMAP, storageHint=StorageHint.IN_MEMORY, concurrent=true)
private Map<List<Object>, Double> lambdas; 
protected ModelParameters(StorageEngine storageEngine) {
super(storageEngine);
}
public Map<List<Object>, Double> getLambdas() {
return lambdas;
}
protected void setLambdas(Map<List<Object>, Double> lambdas) {
this.lambdas = lambdas;
}
}
public static class TrainingParameters extends AbstractClassifier.AbstractTrainingParameters {
private static final long serialVersionUID = 1L;
private int totalIterations=100;
public int getTotalIterations() {
return totalIterations;
}
public void setTotalIterations(int totalIterations) {
this.totalIterations = totalIterations;
}
}
protected MaximumEntropy(TrainingParameters trainingParameters, Configuration configuration) {
super(trainingParameters, configuration);
streamExecutor = new ForkJoinStream(knowledgeBase.getConfiguration().getConcurrencyConfiguration());
}
protected MaximumEntropy(String storageName, Configuration configuration) {
super(storageName, configuration);
streamExecutor = new ForkJoinStream(knowledgeBase.getConfiguration().getConcurrencyConfiguration());
}
private boolean parallelized = true;
protected final ForkJoinStream streamExecutor;
@Override
public boolean isParallelized() {
return parallelized;
}
@Override
public void setParallelized(boolean parallelized) {
this.parallelized = parallelized;
}
@Override
protected void _predict(Dataframe newData) {
_predictDatasetParallel(newData, knowledgeBase.getStorageEngine(), knowledgeBase.getConfiguration().getConcurrencyConfiguration());
}
@Override
public Prediction _predictRecord(Record r) {
Set<Object> classesSet = knowledgeBase.getModelParameters().getClasses();
AssociativeArray predictionScores = new AssociativeArray();
for(Object theClass : classesSet) {
predictionScores.put(theClass, calculateClassScore(r.getX(), theClass));
}
Object predictedClass=getSelectedClassFromClassScores(predictionScores);
Descriptives.normalizeExp(predictionScores);
return new Prediction(predictedClass, predictionScores);
}
@Override
protected void _fit(Dataframe trainingData) {
ModelParameters modelParameters = knowledgeBase.getModelParameters();
int n = trainingData.size();
Map<List<Object>, Double> lambdas = modelParameters.getLambdas();
Set<Object> classesSet = modelParameters.getClasses();
double Cmax = 0.0; 
for(Record r : trainingData) {
Object theClass=r.getY();
classesSet.add(theClass);
int activeFeatures=(int) r.getX().values().stream().filter(e -> e !=null && TypeInference.toDouble(e) > 0.0).count();
if(activeFeatures>Cmax) {
Cmax=activeFeatures;
}
}
StorageEngine storageEngine = knowledgeBase.getStorageEngine();
Map<List<Object>, Double> tmp_EpFj_observed = storageEngine.getBigMap("tmp_EpFj_observed", (Class<List<Object>>)(Class<?>)List.class, Double.class, MapType.HASHMAP, StorageHint.IN_MEMORY, true, true);
streamExecutor.forEach(StreamMethods.stream(trainingData.getXDataTypes().keySet().stream(), isParallelized()), feature -> {
for(Object theClass : classesSet) {
List<Object> featureClassTuple = Arrays.asList(feature, theClass);
tmp_EpFj_observed.put(featureClassTuple, 0.0);
lambdas.put(featureClassTuple, 0.0);
}
});
double increment = 1.0/n; 
streamExecutor.forEach(StreamMethods.stream(trainingData.stream(), isParallelized()), r -> {
Object theClass = r.getY();
for(Map.Entry<Object, Object> entry : r.getX().entrySet()) {
Double occurrences=TypeInference.toDouble(entry.getValue());
if (occurrences!=null && occurrences>0.0) {
Object feature = entry.getKey();
List<Object> featureClassTuple = Arrays.asList(feature, theClass);
synchronized(tmp_EpFj_observed) {
tmp_EpFj_observed.put(featureClassTuple, tmp_EpFj_observed.get(featureClassTuple) + increment);
}
}
}
});
IIS(trainingData, tmp_EpFj_observed, Cmax);
storageEngine.dropBigMap("tmp_EpFj_observed", tmp_EpFj_observed);
}
private void IIS(Dataframe trainingData, Map<List<Object>, Double> EpFj_observed, double Cmax) {
ModelParameters modelParameters = knowledgeBase.getModelParameters();
int totalIterations = knowledgeBase.getTrainingParameters().getTotalIterations();
Set<Object> classesSet = modelParameters.getClasses();
Map<List<Object>, Double> lambdas = modelParameters.getLambdas();
int n = trainingData.size();
StorageEngine storageEngine = knowledgeBase.getStorageEngine();
for(int iteration=0;iteration<totalIterations;++iteration) {
logger.debug("Iteration {}", iteration);
Map<List<Object>, Double> tmp_EpFj_model = storageEngine.getBigMap("tmp_EpFj_model", (Class<List<Object>>)(Class<?>)List.class, Double.class, MapType.HASHMAP, StorageHint.IN_MEMORY, false, true);
streamExecutor.forEach(StreamMethods.stream(trainingData.stream(), isParallelized()), r -> { 
AssociativeArray classScores = new AssociativeArray();
AssociativeArray xData = r.getX();
for(Object theClass : classesSet) {
double score = calculateClassScore(xData, theClass);
classScores.put(theClass, score);
}
Descriptives.normalizeExp(classScores);
for(Map.Entry<Object, Object> entry : classScores.entrySet()) {
Object theClass = entry.getKey();
Double score = TypeInference.toDouble(entry.getValue());
double probabilityFraction = score/n;
synchronized(tmp_EpFj_model) {
for(Map.Entry<Object, Object> entry2 : r.getX().entrySet()) {
Double occurrences=TypeInference.toDouble(entry2.getValue());
if(occurrences==null || occurrences==0.0) {
continue;
}
Object feature = entry2.getKey();
List<Object> featureClassTuple = Arrays.asList(feature, theClass);
tmp_EpFj_model.put(featureClassTuple, tmp_EpFj_model.getOrDefault(featureClassTuple, 0.0) + probabilityFraction);
}
}
}
});
AtomicBoolean infiniteValuesDetected = new AtomicBoolean(false);
streamExecutor.forEach(StreamMethods.stream(tmp_EpFj_model.entrySet().stream(), isParallelized()), featureClassCounts -> {
List<Object> tp = featureClassCounts.getKey();
Double EpFj_observed_value = EpFj_observed.get(tp);
Double EpFj_model_value = featureClassCounts.getValue();
if(Math.abs(EpFj_observed_value-EpFj_model_value)<=1e-8) {
}
else if(EpFj_observed_value==0.0) {
lambdas.put(tp, Double.NEGATIVE_INFINITY);
infiniteValuesDetected.set(true);
}
else if(EpFj_model_value==0.0) {
lambdas.put(tp, Double.POSITIVE_INFINITY);
infiniteValuesDetected.set(true);
}
else {
double deltaJ = Math.log(EpFj_observed_value/EpFj_model_value)/Cmax;
double newValue = lambdas.get(tp) + deltaJ;
lambdas.put(tp, newValue); 
}
});
if(infiniteValuesDetected.get()) {
Double minimumNonInfiniteLambdaWeight = streamExecutor.min(StreamMethods.stream(lambdas.values().stream(), isParallelized()).filter(v -> Double.isFinite(v)), Double::compare).get();
Double maximumNonInfiniteLambdaWeight = streamExecutor.max(StreamMethods.stream(lambdas.values().stream(), isParallelized()).filter(v -> Double.isFinite(v)), Double::compare).get();
streamExecutor.forEach(StreamMethods.stream(lambdas.entrySet().stream(), isParallelized()), e -> {
List<Object> featureClass = e.getKey();
Double value = e.getValue();
if(Double.isInfinite(value)) {
if(value<0.0) { 
lambdas.put(featureClass, minimumNonInfiniteLambdaWeight);
}
else { 
lambdas.put(featureClass, maximumNonInfiniteLambdaWeight);
}
}
});
}
storageEngine.dropBigMap("tmp_EpFj_model", tmp_EpFj_model);
}
}
private Double calculateClassScore(AssociativeArray x, Object theClass) {
double score = 0;
Map<List<Object>, Double> lambdas = knowledgeBase.getModelParameters().getLambdas();
for(Map.Entry<Object, Object> entry : x.entrySet()) {
Double value = TypeInference.toDouble(entry.getValue());
if(value==null || value==0.0) {
continue; 
}
Object feature = entry.getKey();
List<Object> featureClassTuple = Arrays.asList(feature, theClass);
Double lambdaWeight = lambdas.get(featureClassTuple);
if(lambdaWeight!=null) {
score+=lambdaWeight;
}
}
return score;
}
}
